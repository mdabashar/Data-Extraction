{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import all necessary packages\n",
    "from docx import Document\n",
    "import docx\n",
    "import os\n",
    "from docx.document import Document as doctwo\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.table import _Cell, Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_Source = 'C:\\\\Users\\\\User\\\\OneDrive - Queensland University of Technology\\\\converted_pdfs\\\\'\n",
    "BASE_Distination = 'C:\\\\Users\\\\User\\\\OneDrive - Queensland University of Technology\\\\converted_pdfs\\\\DecomposedDocs\\\\CloudStore\\\\'\n",
    "\n",
    "FileList = 'C:\\\\Users\\\\User\\\\OneDrive - Queensland University of Technology\\\\JupyterPythonQUT\\\\DoR\\\\Stage2\\\\FromPrudence\\\\Coal or mineral reports metadata_sent.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl = pd.read_excel(FileList)\n",
    "df_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl['report_batch'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl = df_fl[df_fl['report_batch']=='CloudStor']\n",
    "df_fl.reset_index(drop=True)\n",
    "len(df_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define decompositon class and its functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decompose_document:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # get rels (image id and image name pairs)\n",
    "    def get_rId_image_name_pairs(self, document):\n",
    "        rels = {}\n",
    "        for r in document.part.rels.values():\n",
    "            if isinstance(r._target, docx.parts.image.ImagePart):\n",
    "                rels[r.rId] = os.path.basename(r._target.partname)\n",
    "        return rels\n",
    "\n",
    "    # check if current block is image\n",
    "    def check_for_image(self, block, rels):\n",
    "        for rId in rels:\n",
    "            if rId in block._p.xml:\n",
    "                return rels[rId]\n",
    "        return None\n",
    "    \n",
    "    # get image row to add into dataframe\n",
    "    def add_image_to_row(self, row, file_loc, image_name):\n",
    "        if 'Image Locations' in row.keys():\n",
    "            row['Image Locations']+='|||'+os.path.join(file_loc.split('\\\\')[-1].split('.')[0], image_name)\n",
    "        else:\n",
    "            row['Image Locations']=os.path.join(file_loc.split('\\\\')[-1].split('.')[0], image_name)\n",
    "        return row\n",
    "    \n",
    "    def add_image_desc_to_row(self, row, block):\n",
    "        if 'Image Descriptions' in row.keys():\n",
    "            row['Image Descriptions']+='|||'+block.text\n",
    "        else:\n",
    "            row['Image Descriptions']=block.text\n",
    "        return row\n",
    "    \n",
    "    def add_text_to_row(self, row, block):\n",
    "        if 'Texts' in row.keys():\n",
    "            row['Texts']+='|||'+block.text\n",
    "        else:\n",
    "            row['Texts']=block.text\n",
    "        return row\n",
    "    \n",
    "    def add_table_loc_to_row(self, row, file_loc, table_seq):\n",
    "        if 'Table Locations' in row.keys():\n",
    "            row['Table Locations']+='|||'+os.path.join(file_loc.split('\\\\')[-1].split('.')[0],'table{}.csv'.format(table_seq))\n",
    "        else:\n",
    "            row['Table Locations']=os.path.join(file_loc.split('\\\\')[-1].split('.')[0],'table{}.csv'.format(table_seq))\n",
    "        return row\n",
    "    \n",
    "    def add_table_desc_to_row(self, row, block):\n",
    "        if 'Table Descriptions' in row.keys():\n",
    "            row['Table Descriptions']+='|||'+block.text\n",
    "        else:\n",
    "            row['Table Descriptions']=block.text\n",
    "        return row\n",
    "\n",
    "    ##This function extracts the tables and paragraphs from the document object\n",
    "    def iter_block_items(self, parent):\n",
    "        \"\"\"\n",
    "        Yield each paragraph and table child within *parent*, in document order.\n",
    "        Each returned value is an instance of either Table or Paragraph. *parent*\n",
    "        would most commonly be a reference to a main Document object, but\n",
    "        also works for a _Cell object, which itself can contain paragraphs and tables.\n",
    "        \"\"\"\n",
    "        if isinstance(parent, doctwo):\n",
    "            parent_elm = parent.element.body\n",
    "        elif isinstance(parent, _Cell):\n",
    "            parent_elm = parent._tc\n",
    "        else:\n",
    "            raise ValueError(\"something's not right\")\n",
    "\n",
    "        for child in parent_elm.iterchildren():\n",
    "            if isinstance(child, CT_P):\n",
    "                yield Paragraph(child, parent)\n",
    "            elif isinstance(child, CT_Tbl):\n",
    "                yield Table(child, parent)\n",
    "\n",
    "    def fill_columns(self, df_adoc, doc_loc):\n",
    "        if not ('Doc Location' in df_adoc.columns):\n",
    "            df_adoc['Doc Location'] = [doc_loc for idx in df_adoc.index]\n",
    "        if not ('Heading Seq' in df_adoc.columns):\n",
    "            df_adoc['Heading Seq'] = [np.nan for idx in df_adoc.index]\n",
    "        if not ('Heading' in df_adoc.columns):\n",
    "            df_adoc['Heading'] = [np.nan for idx in df_adoc.index]\n",
    "        if not ('Texts' in df_adoc.columns):\n",
    "            df_adoc['Texts'] = [np.nan for idx in df_adoc.index]\n",
    "        if not ('Image Descriptions' in df_adoc.columns):\n",
    "            df_adoc['Image Descriptions'] = [np.nan for idx in df_adoc.index]\n",
    "        if not ('Image Locations' in df_adoc.columns):\n",
    "            df_adoc['Image Locations'] = [np.nan for idx in df_adoc.index]\n",
    "        if not ('Table Locations' in df_adoc.columns):\n",
    "            df_adoc['Table Locations'] = [np.nan for idx in df_adoc.index]\n",
    "        if not ('Table Descriptions' in df_adoc.columns):\n",
    "            df_adoc['Table Descriptions'] = [np.nan for idx in df_adoc.index]\n",
    "        \n",
    "        df_adoc = df_adoc[['Doc Location', 'Heading Seq', 'Heading', 'Texts', 'Image Descriptions', 'Image Locations', 'Table Descriptions', 'Table Locations']]\n",
    "        \n",
    "        return df_adoc\n",
    "                \n",
    "    # This function splits each data block in a docx and tags them for decomposition\n",
    "    def decompose_docx(self, base, file_loc):\n",
    "        document = Document(base+file_loc)\n",
    "        rels = self.get_rId_image_name_pairs(document)\n",
    "        heading_seq = 0\n",
    "        image_active = False\n",
    "        table_active = False\n",
    "        table_seq = 0\n",
    "        rows = []\n",
    "        row = {'Heading Seq':heading_seq}\n",
    "        for block in self.iter_block_items(document):\n",
    "            \n",
    "            if 'text' in str(block): # the block is text (heading of paragraph)\n",
    "                image_name = self.check_for_image(block, rels)\n",
    "                if image_name: # the block is image\n",
    "                    print('-[Image]', os.path.join(file_loc.split('\\\\')[-1].split('.')[0], image_name))\n",
    "                    image_active = True\n",
    "                    row = self.add_image_to_row(row, file_loc, image_name)\n",
    "                elif block.style.name.startswith('Heading'):\n",
    "                    print('[Heading {}]'.format(heading_seq), block.text)\n",
    "                    if 'Heading' in row.keys():\n",
    "                        rows.append(row)\n",
    "                        heading_seq+=1\n",
    "                        row = {'Heading Seq':heading_seq}\n",
    "                        row['Heading']=block.text\n",
    "                    else:\n",
    "                        row['Heading']=block.text\n",
    "                        heading_seq+=1\n",
    "                elif block.text!='':\n",
    "                    print('-', block.text)\n",
    "                    if image_active:\n",
    "                        #print('-', block.text)\n",
    "                        row = self.add_image_desc_to_row(row, block)\n",
    "                        image_active = False\n",
    "                    if table_active:\n",
    "                        #print('-', block.text)\n",
    "                        row = self.add_table_desc_to_row(row, block)\n",
    "                        table_active = False\n",
    "                    else:\n",
    "                        #print('-', block.text)\n",
    "                        row = self.add_text_to_row(row, block)\n",
    "            else: # the block is table\n",
    "                print('-[Table]')\n",
    "                table_active = True\n",
    "                row = self.add_table_loc_to_row(row, file_loc, table_seq)\n",
    "                table_seq+=1\n",
    "        rows.append(row)\n",
    "        df_adoc = pd.DataFrame.from_records(rows)\n",
    "        df_adoc = self.fill_columns(df_adoc, file_loc)\n",
    "        \n",
    "        return df_adoc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do decomposition on files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exist(drive_path, file_name):\n",
    "    drive_items = os.listdir(drive_path)\n",
    "    return file_name in drive_items\n",
    "\n",
    "def write_elapsed_time(drive_path, elapsed_time, task_name):\n",
    "    string = '===================================\\n'\n",
    "    string += 'Task Name: '+task_name+'\\n'\n",
    "    string += 'Elapsed Time: '+elapsed_time+'\\n'\n",
    "    string += 'Date time at the end of task: {}\\n'.format(datetime.today())\n",
    "    with open(drive_path+'elapsed_times.txt', 'a+') as FO:\n",
    "        FO.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_end = 0\n",
    "for i in range((int(len(df_fl)/100))):\n",
    "    range_end = (i+1)*100\n",
    "    if check_exist(BASE_Distination, str(range_end-100+1)+'_'+str(range_end)+'_decom.csv'):\n",
    "        print('File {}_decom.csv Exists'.format(str(range_end-100+1)+'_'+str(range_end)))\n",
    "    else:\n",
    "        break\n",
    "print('---Current file~'+str(range_end-100+1)+'_'+str(range_end)+'_decom.csv')\n",
    "\n",
    "dc = decompose_document()\n",
    "count = 0\n",
    "for idx in df_fl.index:\n",
    "    count+=1\n",
    "    if count < range_end-100+1:\n",
    "        continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"Date time: {} at the start of file [{}]\".format(datetime.today(), range_end))\n",
    "\n",
    "    file_loc = df_fl.loc[idx, 'Datagroup']+'\\\\'+str(int(df_fl.loc[idx, 'batch']))+'\\\\'+df_fl.loc[idx, 'C'].split('.')[0]+'.docx'\n",
    "    df_adoc = dc.decompose_docx(BASE_Source, file_loc)\n",
    "    #df_adoc = dc.decompose_docx('C:\\\\Users\\\\User\\\\Downloads\\\\OneDrive_2022-12-13\\\\gsq identified reports\\\\', 'cr085525_cr_85525_1.docx')\n",
    "    #df_adoc = dc.decompose_docx('C:\\\\Users\\\\User\\\\Downloads\\\\OneDrive_2022-12-13\\\\gsq identified reports\\\\', 'cr113547_cr_113547_1.docx')\n",
    "    # todo\n",
    "    break\n",
    "\n",
    "    if count%100==0:\n",
    "        elapsed_time = str(timedelta(seconds=time.time() - start_time))\n",
    "        print('Elapsed Time: {}'.format(elapsed_time))\n",
    "        write_elapsed_time(BASE_Distination, elapsed_time, '{}_out.csv'.format(start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adoc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
